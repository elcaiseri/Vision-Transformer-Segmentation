{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.011512,
     "end_time": "2021-03-27T07:55:53.738839",
     "exception": false,
     "start_time": "2021-03-27T07:55:53.727327",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Description\n",
    "This kernel provides a starter Pytorch code for inference that performs dividing the images into tiles([based on this kernel](https://www.kaggle.com/iafoss/256x256-images)), selection of tiles with tissue, evaluation of the predictions of multiple models with TTA, combining the tile masks back into image level masks, and conversion into RLE. The inference is performed based on models trained in the [fast.ai starter kernel](https://www.kaggle.com/iafoss/hubmap-fast-ai-starter), provided by me. I hope it will help you to get started with this competition.\n",
    "\n",
    "* Update (12/4): Fix problem with submission to private LB using **rasterio** (thanks to @leighplt for suggesting it in [his kernel](https://www.kaggle.com/leighplt/pytorch-fcn-resnet50)). For the prediction on the public part of the test set the predictions are identical except one of the images, where the new method predicts a mask different by several pixels, but the LB is the same. I think the tiles loaded by rasterio may be slightly different from ones loaded by tifffile for some image compressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-03-27T07:55:53.766848Z",
     "iopub.status.busy": "2021-03-27T07:55:53.766174Z",
     "iopub.status.idle": "2021-03-27T07:55:57.025601Z",
     "shell.execute_reply": "2021-03-27T07:55:57.025089Z"
    },
    "papermill": {
     "duration": 3.276306,
     "end_time": "2021-03-27T07:55:57.025711",
     "exception": false,
     "start_time": "2021-03-27T07:55:53.749405",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import tifffile as tiff\n",
    "import cv2\n",
    "import os\n",
    "import gc\n",
    "from tqdm.notebook import tqdm\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "\n",
    "from fastai.vision.all import *\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-27T07:55:57.056021Z",
     "iopub.status.busy": "2021-03-27T07:55:57.055490Z",
     "iopub.status.idle": "2021-03-27T07:55:57.064881Z",
     "shell.execute_reply": "2021-03-27T07:55:57.064451Z"
    },
    "papermill": {
     "duration": 0.027809,
     "end_time": "2021-03-27T07:55:57.064966",
     "exception": false,
     "start_time": "2021-03-27T07:55:57.037157",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sz = 256   #the size of tiles\n",
    "reduce = 4 #reduce the original images by 4 times\n",
    "TH = 0.43  #threshold for positive predictions\n",
    "DATA = '../input/hubmap-kidney-segmentation/test/'\n",
    "MODELS1 = [f'../input/hubmap-fast-ai-starter/model_{i}.pth' for i in range(4)]\n",
    "MODELS2 = [f'../input/hubmap-pytorch-starter-vit-segmentation-train/FOLD-0-model.pth']\n",
    "MODELS = MODELS1 + MODELS2\n",
    "df_sample = pd.read_csv('../input/hubmap-kidney-segmentation/sample_submission.csv')\n",
    "bs = 64\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-27T07:55:57.092182Z",
     "iopub.status.busy": "2021-03-27T07:55:57.091460Z",
     "iopub.status.idle": "2021-03-27T07:55:57.098231Z",
     "shell.execute_reply": "2021-03-27T07:55:57.097520Z"
    },
    "papermill": {
     "duration": 0.022329,
     "end_time": "2021-03-27T07:55:57.098352",
     "exception": false,
     "start_time": "2021-03-27T07:55:57.076023",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../input/hubmap-fast-ai-starter/model_0.pth',\n",
       " '../input/hubmap-fast-ai-starter/model_1.pth',\n",
       " '../input/hubmap-fast-ai-starter/model_2.pth',\n",
       " '../input/hubmap-fast-ai-starter/model_3.pth',\n",
       " '../input/hubmap-pytorch-starter-vit-segmentation-train/FOLD-0-model.pth']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-27T07:55:57.129948Z",
     "iopub.status.busy": "2021-03-27T07:55:57.129296Z",
     "iopub.status.idle": "2021-03-27T07:55:57.132591Z",
     "shell.execute_reply": "2021-03-27T07:55:57.132181Z"
    },
    "papermill": {
     "duration": 0.022075,
     "end_time": "2021-03-27T07:55:57.132678",
     "exception": false,
     "start_time": "2021-03-27T07:55:57.110603",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    data = 256 #512\n",
    "    debug=False\n",
    "    apex=False\n",
    "    print_freq=100\n",
    "    num_workers=4\n",
    "    img_size=256 # appropriate input size for encoder \n",
    "    scheduler='CosineAnnealingWarmRestarts' # ['ReduceLROnPlateau', 'CosineAnnealingLR', 'CosineAnnealingWarmRestarts']\n",
    "    epoch=5 # Change epochs\n",
    "    criterion= 'Lovasz' #'DiceBCELoss' # ['DiceLoss', 'Hausdorff', 'Lovasz']\n",
    "    base_model='Unet' # ['Unet']\n",
    "    encoder = 'vit' # ['efficientnet-b5'] or other encoders from smp\n",
    "    lr=1e-4\n",
    "    min_lr=1e-6\n",
    "    batch_size=10\n",
    "    weight_decay=1e-6\n",
    "    gradient_accumulation_steps=1\n",
    "    seed=1911\n",
    "    n_fold=5\n",
    "    trn_fold=[0, 1, 2, 3, 4]\n",
    "    train=True\n",
    "    inference=False\n",
    "    optimizer = 'Adam'\n",
    "    T_0=10\n",
    "    # N=5 \n",
    "    # M=9\n",
    "    T_max=10\n",
    "    #factor=0.2\n",
    "    #patience=4\n",
    "    #eps=1e-6\n",
    "    smoothing=1\n",
    "    in_channels=3\n",
    "    vit_blocks=4 #[8, 12]\n",
    "    vit_linear=512 #1024\n",
    "    classes=1\n",
    "\n",
    "\n",
    "main_dir = '../input/hubmap-256x256'\n",
    "train_dir = '../input/hubmap-256x256/train'\n",
    "masks_dir = '../input/hubmap-256x256/masks'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.011188,
     "end_time": "2021-03-27T07:55:57.154979",
     "exception": false,
     "start_time": "2021-03-27T07:55:57.143791",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-03-27T07:55:57.192710Z",
     "iopub.status.busy": "2021-03-27T07:55:57.192041Z",
     "iopub.status.idle": "2021-03-27T07:55:57.194380Z",
     "shell.execute_reply": "2021-03-27T07:55:57.194872Z"
    },
    "papermill": {
     "duration": 0.028914,
     "end_time": "2021-03-27T07:55:57.194978",
     "exception": false,
     "start_time": "2021-03-27T07:55:57.166064",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#functions to convert encoding to mask and mask to encoding\n",
    "def enc2mask(encs, shape):\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for m,enc in enumerate(encs):\n",
    "        if isinstance(enc,np.float) and np.isnan(enc): continue\n",
    "        s = enc.split()\n",
    "        for i in range(len(s)//2):\n",
    "            start = int(s[2*i]) - 1\n",
    "            length = int(s[2*i+1])\n",
    "            img[start:start+length] = 1 + m\n",
    "    return img.reshape(shape).T\n",
    "\n",
    "def mask2enc(mask, n=1):\n",
    "    pixels = mask.T.flatten()\n",
    "    encs = []\n",
    "    for i in range(1,n+1):\n",
    "        p = (pixels == i).astype(np.int8)\n",
    "        if p.sum() == 0: encs.append(np.nan)\n",
    "        else:\n",
    "            p = np.concatenate([[0], p, [0]])\n",
    "            runs = np.where(p[1:] != p[:-1])[0] + 1\n",
    "            runs[1::2] -= runs[::2]\n",
    "            encs.append(' '.join(str(x) for x in runs))\n",
    "    return encs\n",
    "\n",
    "#https://www.kaggle.com/bguberfain/memory-aware-rle-encoding\n",
    "#with transposed mask\n",
    "def rle_encode_less_memory(img):\n",
    "    #the image should be transposed\n",
    "    pixels = img.T.flatten()\n",
    "    \n",
    "    # This simplified method requires first and last pixel to be zero\n",
    "    pixels[0] = 0\n",
    "    pixels[-1] = 0\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 2\n",
    "    runs[1::2] -= runs[::2]\n",
    "    \n",
    "    return ' '.join(str(x) for x in runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-27T07:55:57.244515Z",
     "iopub.status.busy": "2021-03-27T07:55:57.230370Z",
     "iopub.status.idle": "2021-03-27T07:55:57.246747Z",
     "shell.execute_reply": "2021-03-27T07:55:57.246350Z"
    },
    "papermill": {
     "duration": 0.040566,
     "end_time": "2021-03-27T07:55:57.246832",
     "exception": false,
     "start_time": "2021-03-27T07:55:57.206266",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/iafoss/256x256-images\n",
    "mean = np.array([0.65459856,0.48386562,0.69428385])\n",
    "std = np.array([0.15167958,0.23584107,0.13146145])\n",
    "\n",
    "s_th = 40  #saturation blancking threshold\n",
    "p_th = 1000*(sz//256)**2 #threshold for the minimum number of pixels\n",
    "identity = rasterio.Affine(1, 0, 0, 0, 1, 0)\n",
    "\n",
    "def img2tensor(img,dtype:np.dtype=np.float32):\n",
    "    if img.ndim==2 : img = np.expand_dims(img,2)\n",
    "    img = np.transpose(img,(2,0,1))\n",
    "    return torch.from_numpy(img.astype(dtype, copy=False))\n",
    "\n",
    "class HuBMAPDataset(Dataset):\n",
    "    def __init__(self, idx, sz=sz, reduce=reduce):\n",
    "        self.data = rasterio.open(os.path.join(DATA,idx+'.tiff'), transform = identity,\n",
    "                                 num_threads='all_cpus')\n",
    "        # some images have issues with their format \n",
    "        # and must be saved correctly before reading with rasterio\n",
    "        if self.data.count != 3:\n",
    "            subdatasets = self.data.subdatasets\n",
    "            self.layers = []\n",
    "            if len(subdatasets) > 0:\n",
    "                for i, subdataset in enumerate(subdatasets, 0):\n",
    "                    self.layers.append(rasterio.open(subdataset))\n",
    "        self.shape = self.data.shape\n",
    "        self.reduce = reduce\n",
    "        self.sz = reduce*sz\n",
    "        self.pad0 = (self.sz - self.shape[0]%self.sz)%self.sz\n",
    "        self.pad1 = (self.sz - self.shape[1]%self.sz)%self.sz\n",
    "        self.n0max = (self.shape[0] + self.pad0)//self.sz\n",
    "        self.n1max = (self.shape[1] + self.pad1)//self.sz\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n0max*self.n1max\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # the code below may be a little bit difficult to understand,\n",
    "        # but the thing it does is mapping the original image to\n",
    "        # tiles created with adding padding, as done in\n",
    "        # https://www.kaggle.com/iafoss/256x256-images ,\n",
    "        # and then the tiles are loaded with rasterio\n",
    "        # n0,n1 - are the x and y index of the tile (idx = n0*self.n1max + n1)\n",
    "        n0,n1 = idx//self.n1max, idx%self.n1max\n",
    "        # x0,y0 - are the coordinates of the lower left corner of the tile in the image\n",
    "        # negative numbers correspond to padding (which must not be loaded)\n",
    "        x0,y0 = -self.pad0//2 + n0*self.sz, -self.pad1//2 + n1*self.sz\n",
    "        # make sure that the region to read is within the image\n",
    "        p00,p01 = max(0,x0), min(x0+self.sz,self.shape[0])\n",
    "        p10,p11 = max(0,y0), min(y0+self.sz,self.shape[1])\n",
    "        img = np.zeros((self.sz,self.sz,3),np.uint8)\n",
    "        # mapping the loade region to the tile\n",
    "        if self.data.count == 3:\n",
    "            img[(p00-x0):(p01-x0),(p10-y0):(p11-y0)] = np.moveaxis(self.data.read([1,2,3],\n",
    "                window=Window.from_slices((p00,p01),(p10,p11))), 0, -1)\n",
    "        else:\n",
    "            for i,layer in enumerate(self.layers):\n",
    "                img[(p00-x0):(p01-x0),(p10-y0):(p11-y0),i] =\\\n",
    "                  layer.read(1,window=Window.from_slices((p00,p01),(p10,p11)))\n",
    "        \n",
    "        if self.reduce != 1:\n",
    "            img = cv2.resize(img,(self.sz//reduce,self.sz//reduce),\n",
    "                             interpolation = cv2.INTER_AREA)\n",
    "        #check for empty imges\n",
    "        hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "        h,s,v = cv2.split(hsv)\n",
    "        if (s>s_th).sum() <= p_th or img.sum() <= p_th:\n",
    "            #images with -1 will be skipped\n",
    "            return img2tensor((img/255.0 - mean)/std), -1\n",
    "        else: return img2tensor((img/255.0 - mean)/std), idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-27T07:55:57.284565Z",
     "iopub.status.busy": "2021-03-27T07:55:57.283825Z",
     "iopub.status.idle": "2021-03-27T07:55:57.286593Z",
     "shell.execute_reply": "2021-03-27T07:55:57.286087Z"
    },
    "papermill": {
     "duration": 0.0283,
     "end_time": "2021-03-27T07:55:57.286676",
     "exception": false,
     "start_time": "2021-03-27T07:55:57.258376",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#iterator like wrapper that returns predicted masks\n",
    "class Model_pred:\n",
    "    def __init__(self, models, dl, tta:bool=True, half:bool=False):\n",
    "        self.models = models\n",
    "        self.dl = dl\n",
    "        self.tta = tta\n",
    "        self.half = half\n",
    "        \n",
    "    def __iter__(self):\n",
    "        count=0\n",
    "        with torch.no_grad():\n",
    "            for x,y in iter(self.dl):\n",
    "                if ((y>=0).sum() > 0): #exclude empty images\n",
    "                    x = x[y>=0].to(device)\n",
    "                    y = y[y>=0]\n",
    "                    if self.half: x = x.half()\n",
    "                    py = None\n",
    "                    for model in self.models:\n",
    "                        p = model(x)\n",
    "                        p = torch.sigmoid(p).detach()\n",
    "                        if py is None: py = p\n",
    "                        else: py += p\n",
    "                    if self.tta:\n",
    "                        #x,y,xy flips as TTA\n",
    "                        flips = [[-1],[-2],[-2,-1]]\n",
    "                        for f in flips:\n",
    "                            xf = torch.flip(x,f)\n",
    "                            for model in self.models:\n",
    "                                p = model(xf)\n",
    "                                p = torch.flip(p,f)\n",
    "                                py += torch.sigmoid(p).detach()\n",
    "                        py /= (1+len(flips))        \n",
    "                    py /= len(self.models)\n",
    "\n",
    "                    py = F.upsample(py, scale_factor=reduce, mode=\"bilinear\")\n",
    "                    py = py.permute(0,2,3,1).float().cpu()\n",
    "                    \n",
    "                    batch_size = len(py)\n",
    "                    for i in range(batch_size):\n",
    "                        yield py[i],y[i]\n",
    "                        count += 1\n",
    "                    \n",
    "    def __len__(self):\n",
    "        return len(self.dl.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.011469,
     "end_time": "2021-03-27T07:55:57.309734",
     "exception": false,
     "start_time": "2021-03-27T07:55:57.298265",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-03-27T07:55:57.361426Z",
     "iopub.status.busy": "2021-03-27T07:55:57.345838Z",
     "iopub.status.idle": "2021-03-27T07:55:57.370657Z",
     "shell.execute_reply": "2021-03-27T07:55:57.370235Z"
    },
    "papermill": {
     "duration": 0.049638,
     "end_time": "2021-03-27T07:55:57.370744",
     "exception": false,
     "start_time": "2021-03-27T07:55:57.321106",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FPN(nn.Module):\n",
    "    def __init__(self, input_channels:list, output_channels:list):\n",
    "        super().__init__()\n",
    "        self.convs = nn.ModuleList(\n",
    "            [nn.Sequential(nn.Conv2d(in_ch, out_ch*2, kernel_size=3, padding=1),\n",
    "             nn.ReLU(inplace=True), nn.BatchNorm2d(out_ch*2),\n",
    "             nn.Conv2d(out_ch*2, out_ch, kernel_size=3, padding=1))\n",
    "            for in_ch, out_ch in zip(input_channels, output_channels)])\n",
    "        \n",
    "    def forward(self, xs:list, last_layer):\n",
    "        hcs = [F.interpolate(c(x),scale_factor=2**(len(self.convs)-i),mode='bilinear') \n",
    "               for i,(c,x) in enumerate(zip(self.convs, xs))]\n",
    "        hcs.append(last_layer)\n",
    "        return torch.cat(hcs, dim=1)\n",
    "\n",
    "class UnetBlock(Module):\n",
    "    def __init__(self, up_in_c:int, x_in_c:int, nf:int=None, blur:bool=False,\n",
    "                 self_attention:bool=False, **kwargs):\n",
    "        super().__init__()\n",
    "        self.shuf = PixelShuffle_ICNR(up_in_c, up_in_c//2, blur=blur, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(x_in_c)\n",
    "        ni = up_in_c//2 + x_in_c\n",
    "        nf = nf if nf is not None else max(up_in_c//2,32)\n",
    "        self.conv1 = ConvLayer(ni, nf, norm_type=None, **kwargs)\n",
    "        self.conv2 = ConvLayer(nf, nf, norm_type=None,\n",
    "            xtra=SelfAttention(nf) if self_attention else None, **kwargs)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, up_in:Tensor, left_in:Tensor) -> Tensor:\n",
    "        s = left_in\n",
    "        up_out = self.shuf(up_in)\n",
    "        cat_x = self.relu(torch.cat([up_out, self.bn(s)], dim=1))\n",
    "        return self.conv2(self.conv1(cat_x))\n",
    "        \n",
    "class _ASPPModule(nn.Module):\n",
    "    def __init__(self, inplanes, planes, kernel_size, padding, dilation, groups=1):\n",
    "        super().__init__()\n",
    "        self.atrous_conv = nn.Conv2d(inplanes, planes, kernel_size=kernel_size,\n",
    "                stride=1, padding=padding, dilation=dilation, bias=False, groups=groups)\n",
    "        self.bn = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self._init_weight()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.atrous_conv(x)\n",
    "        x = self.bn(x)\n",
    "\n",
    "        return self.relu(x)\n",
    "\n",
    "    def _init_weight(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                torch.nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "class ASPP(nn.Module):\n",
    "    def __init__(self, inplanes=512, mid_c=256, dilations=[6, 12, 18, 24], out_c=None):\n",
    "        super().__init__()\n",
    "        self.aspps = [_ASPPModule(inplanes, mid_c, 1, padding=0, dilation=1)] + \\\n",
    "            [_ASPPModule(inplanes, mid_c, 3, padding=d, dilation=d,groups=4) for d in dilations]\n",
    "        self.aspps = nn.ModuleList(self.aspps)\n",
    "        self.global_pool = nn.Sequential(nn.AdaptiveMaxPool2d((1, 1)),\n",
    "                        nn.Conv2d(inplanes, mid_c, 1, stride=1, bias=False),\n",
    "                        nn.BatchNorm2d(mid_c), nn.ReLU())\n",
    "        out_c = out_c if out_c is not None else mid_c\n",
    "        self.out_conv = nn.Sequential(nn.Conv2d(mid_c*(2+len(dilations)), out_c, 1, bias=False),\n",
    "                                    nn.BatchNorm2d(out_c), nn.ReLU(inplace=True))\n",
    "        self.conv1 = nn.Conv2d(mid_c*(2+len(dilations)), out_c, 1, bias=False)\n",
    "        self._init_weight()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.global_pool(x)\n",
    "        xs = [aspp(x) for aspp in self.aspps]\n",
    "        x0 = F.interpolate(x0, size=xs[0].size()[2:], mode='bilinear', align_corners=True)\n",
    "        x = torch.cat([x0] + xs, dim=1)\n",
    "        return self.out_conv(x)\n",
    "    \n",
    "    def _init_weight(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                torch.nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-27T07:55:57.412747Z",
     "iopub.status.busy": "2021-03-27T07:55:57.411203Z",
     "iopub.status.idle": "2021-03-27T07:55:57.413594Z",
     "shell.execute_reply": "2021-03-27T07:55:57.414022Z"
    },
    "papermill": {
     "duration": 0.031797,
     "end_time": "2021-03-27T07:55:57.414132",
     "exception": false,
     "start_time": "2021-03-27T07:55:57.382335",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.models.resnet import ResNet, Bottleneck\n",
    "class UneXt50(nn.Module):\n",
    "    def __init__(self, stride=1, **kwargs):\n",
    "        super().__init__()\n",
    "        #encoder\n",
    "        m = ResNet(Bottleneck, [3, 4, 6, 3], groups=32, width_per_group=4)\n",
    "        #m = torch.hub.load('facebookresearch/semi-supervised-ImageNet1K-models',\n",
    "        #                   'resnext50_32x4d_ssl')\n",
    "        self.enc0 = nn.Sequential(m.conv1, m.bn1, nn.ReLU(inplace=True))\n",
    "        self.enc1 = nn.Sequential(nn.MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1),\n",
    "                            m.layer1) #256\n",
    "        self.enc2 = m.layer2 #512\n",
    "        self.enc3 = m.layer3 #1024\n",
    "        self.enc4 = m.layer4 #2048\n",
    "        #aspp with customized dilatations\n",
    "        self.aspp = ASPP(2048,256,out_c=512,dilations=[stride*1,stride*2,stride*3,stride*4])\n",
    "        self.drop_aspp = nn.Dropout2d(0.5)\n",
    "        #decoder\n",
    "        self.dec4 = UnetBlock(512,1024,256)\n",
    "        self.dec3 = UnetBlock(256,512,128)\n",
    "        self.dec2 = UnetBlock(128,256,64)\n",
    "        self.dec1 = UnetBlock(64,64,32)\n",
    "        self.fpn = FPN([512,256,128,64],[16]*4)\n",
    "        self.drop = nn.Dropout2d(0.1)\n",
    "        self.final_conv = ConvLayer(32+16*4, 1, ks=1, norm_type=None, act_cls=None)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        enc0 = self.enc0(x)\n",
    "        enc1 = self.enc1(enc0)\n",
    "        enc2 = self.enc2(enc1)\n",
    "        enc3 = self.enc3(enc2)\n",
    "        enc4 = self.enc4(enc3)\n",
    "        enc5 = self.aspp(enc4)\n",
    "        dec3 = self.dec4(self.drop_aspp(enc5),enc3)\n",
    "        dec2 = self.dec3(dec3,enc2)\n",
    "        dec1 = self.dec2(dec2,enc1)\n",
    "        dec0 = self.dec1(dec1,enc0)\n",
    "        x = self.fpn([enc5, dec3, dec2, dec1], dec0)\n",
    "        x = self.final_conv(self.drop(x))\n",
    "        x = F.interpolate(x,scale_factor=2,mode='bilinear')\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-27T07:55:57.444696Z",
     "iopub.status.busy": "2021-03-27T07:55:57.444185Z",
     "iopub.status.idle": "2021-03-27T07:55:57.707262Z",
     "shell.execute_reply": "2021-03-27T07:55:57.706237Z"
    },
    "papermill": {
     "duration": 0.281656,
     "end_time": "2021-03-27T07:55:57.707416",
     "exception": false,
     "start_time": "2021-03-27T07:55:57.425760",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#! pip install self-attention-cv\n",
    "#!pip install einops\n",
    "cv_path = '../input/selfattentioncv/self-attention-cv-main'\n",
    "ep_path = '../input/einops/einops-master'\n",
    "sys.path.append(cv_path)\n",
    "sys.path.append(ep_path)\n",
    "\n",
    "#from self_attention_cv import ViT\n",
    "from self_attention_cv.transunet import TransUnet\n",
    "\n",
    "class HuBMAPModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HuBMAPModel, self).__init__()\n",
    "        \n",
    "        self.model = TransUnet(in_channels=CFG.in_channels, \n",
    "                               img_dim=CFG.img_size, \n",
    "                               vit_blocks=CFG.vit_blocks, \n",
    "                               vit_dim_linear_mhsa_block=CFG.vit_linear, \n",
    "                               classes=CFG.classes)\n",
    "\n",
    "    \n",
    "    def forward(self, img):\n",
    "        img_segs = self.model(img)\n",
    "        \n",
    "        return img_segs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-27T07:55:57.737885Z",
     "iopub.status.busy": "2021-03-27T07:55:57.737286Z",
     "iopub.status.idle": "2021-03-27T07:56:16.638345Z",
     "shell.execute_reply": "2021-03-27T07:56:16.636936Z"
    },
    "papermill": {
     "duration": 18.918716,
     "end_time": "2021-03-27T07:56:16.638460",
     "exception": false,
     "start_time": "2021-03-27T07:55:57.719744",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = []\n",
    "for path in MODELS:\n",
    "    state_dict = torch.load(path, map_location=torch.device('cpu'))\n",
    "    try:\n",
    "        model = UneXt50() #HuBMAPModel() #UneXt50()\n",
    "        model.load_state_dict(state_dict)\n",
    "    except:\n",
    "        model = HuBMAPModel()\n",
    "        model.load_state_dict(state_dict)\n",
    "        \n",
    "    model.float()\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    models.append(model)\n",
    "\n",
    "del state_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013017,
     "end_time": "2021-03-27T07:56:16.664449",
     "exception": false,
     "start_time": "2021-03-27T07:56:16.651432",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2021-03-27T07:56:16.721613Z",
     "iopub.status.busy": "2021-03-27T07:56:16.702558Z",
     "iopub.status.idle": "2021-03-27T08:11:05.587367Z",
     "shell.execute_reply": "2021-03-27T08:11:05.584091Z"
    },
    "papermill": {
     "duration": 888.909911,
     "end_time": "2021-03-27T08:11:05.587500",
     "exception": false,
     "start_time": "2021-03-27T07:56:16.677589",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d2359cc21fa45cfaec81e1182119a1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "names,preds = [],[]\n",
    "for idx,row in tqdm(df_sample.iterrows(), total=len(df_sample)):\n",
    "    idx = row['id']\n",
    "    ds = HuBMAPDataset(idx)\n",
    "    \n",
    "    #rasterio cannot be used with multiple workers\n",
    "    dl = DataLoader(ds,bs,num_workers=0,shuffle=False,pin_memory=True)\n",
    "    mp = Model_pred(models,dl)\n",
    "    \n",
    "    #generate masks\n",
    "    mask = torch.zeros(len(ds), ds.sz, ds.sz, dtype=torch.int8)\n",
    "    for p,i in iter(mp): mask[i.item()] = p.squeeze(-1) > TH\n",
    "    \n",
    "    #reshape tiled masks into a single mask and crop padding\n",
    "    mask = mask.view(ds.n0max,ds.n1max,ds.sz,ds.sz).\\\n",
    "        permute(0,2,1,3).reshape(ds.n0max*ds.sz,ds.n1max*ds.sz)\n",
    "    mask = mask[ds.pad0//2:-(ds.pad0-ds.pad0//2) if ds.pad0 > 0 else ds.n0max*ds.sz,\n",
    "        ds.pad1//2:-(ds.pad1-ds.pad1//2) if ds.pad1 > 0 else ds.n1max*ds.sz]\n",
    "    \n",
    "    #convert to rle\n",
    "    #https://www.kaggle.com/bguberfain/memory-aware-rle-encoding\n",
    "    rle = rle_encode_less_memory(mask.numpy())\n",
    "    names.append(idx)\n",
    "    preds.append(rle)\n",
    "    del mask, ds, dl\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-27T08:11:05.619471Z",
     "iopub.status.busy": "2021-03-27T08:11:05.618848Z",
     "iopub.status.idle": "2021-03-27T08:11:06.051164Z",
     "shell.execute_reply": "2021-03-27T08:11:06.050091Z"
    },
    "papermill": {
     "duration": 0.449691,
     "end_time": "2021-03-27T08:11:06.051282",
     "exception": false,
     "start_time": "2021-03-27T08:11:05.601591",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'id':names,'predicted':preds})\n",
    "df.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-27T08:11:06.097515Z",
     "iopub.status.busy": "2021-03-27T08:11:06.096831Z",
     "iopub.status.idle": "2021-03-27T08:11:06.100902Z",
     "shell.execute_reply": "2021-03-27T08:11:06.100332Z"
    },
    "papermill": {
     "duration": 0.035984,
     "end_time": "2021-03-27T08:11:06.101039",
     "exception": false,
     "start_time": "2021-03-27T08:11:06.065055",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   id         5 non-null      object\n",
      " 1   predicted  5 non-null      object\n",
      "dtypes: object(2)\n",
      "memory usage: 208.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.013022,
     "end_time": "2021-03-27T08:11:06.128453",
     "exception": false,
     "start_time": "2021-03-27T08:11:06.115431",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 916.982229,
   "end_time": "2021-03-27T08:11:06.849420",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-03-27T07:55:49.867191",
   "version": "2.1.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "115b827e86e3442497f4e2119abab551": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "2ab48e087e3c4b81b174b2bd0e40453d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "31478299238a43bcb6b1004b2a598c69": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_964e3e79e6f048c4baea0d3ea3eff101",
       "max": 5.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_de1b28b82e834dc8af284558cd77667a",
       "value": 5.0
      }
     },
     "3d2359cc21fa45cfaec81e1182119a1e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_31478299238a43bcb6b1004b2a598c69",
        "IPY_MODEL_9d489f1962fb47c0b13bbe1636634c24"
       ],
       "layout": "IPY_MODEL_6671e9af7d4e4d148419eb26b52e36ef"
      }
     },
     "6671e9af7d4e4d148419eb26b52e36ef": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "964e3e79e6f048c4baea0d3ea3eff101": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9d489f1962fb47c0b13bbe1636634c24": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2ab48e087e3c4b81b174b2bd0e40453d",
       "placeholder": "​",
       "style": "IPY_MODEL_115b827e86e3442497f4e2119abab551",
       "value": " 5/5 [14:48&lt;00:00, 177.77s/it]"
      }
     },
     "de1b28b82e834dc8af284558cd77667a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
